---
title: "Compare Vocal Recordings: Feb 2020 Pilot Data"
output: html_notebook
---

This is a continuation of 9_compareGesturesPilot.Rmd. First run all the scripts in part 1 of that notebook.

Current TODO:
Need to fix the textgrids for the region that has sound
Write a function that saves plot of all curves by phrase/condition
Inspect and fix the TextGrids that have issues

# Loading Voice Files

```{r}
library("rPraat")
library("tidyverse")
library("dtw")

path="../data/21_02-study/all_voice/"
ref_freq <- 116.54

# Find pitchTier files for voice recordings
setwd(paste0(path, "pt"))
vpt_filenames <- dir(pattern="\\.PitchTier$")

# Function to load the interpolated pitchtier of each gesture
# pt_name is the filename ending in .PitchTier
get_voice_pt_interp <- function(pt_name) {
  my_pt<-pt.read(paste0(path, "pt/", pt_name))
  # Get the core name of the file, without extension
  core_name<-str_split(pt_name, ".PitchTier")[[1]][1]
  # Get the TextGrid name
  tg_name <- paste0(core_name, "_auto.TextGrid")
  # load the textgrid
  my_tg<-tg.read(paste0(path, "tg/", tg_name))
  # Get the start and end time of the vocalization within the file
  start_time <- my_tg$syll$t2[1]
  end_time <- tail(my_tg$syll$t1, 1)

  interp<-pt.interpolate(my_pt, seq(start_time, end_time, by=0.01))
  #convert frequency from hertz to semitones
  interp$f <- hertz_to_semitones(interp$f, ref_freq)
  
  # Normalize by the mean
  interp$f <- interp$f-mean(interp$f)
  
  return(interp)
}

# All the interpolated pitchtiers
setwd("../../../../notebooks/")
vpts_interp <- lapply(vpt_filenames, get_voice_pt_interp)

# Raw filenames for voice voles
voices <- do.call("rbind", lapply(vpt_filenames, function(elt) {
  str_split(elt, '_filtre.PitchTier')[[1]][1]
}))[,1]

# Info on voice files
voice_data <- do.call("rbind", lapply(voices, function(elt) {
  parts<-str_split(elt, '-')[[1]]
  ref_phrase<-parts[3]
  phrase_id <- str_split(ref_phrase, '_')[[1]][1]
  tibble(subject=parts[1], type=parts[2], pid=phrase_id, order=parts[4], filename=elt, 
         phrase=ref_phrase)
})) %>% mutate(id = row_number())

```


# Comparing

Load scripts in part 1 of 9_compareGesturesPilot2.Rmd to have access to reference data

```{r}
# Given id of voice recording, returns the data for both the recording and its reference
# for comparison and plotting. Output tibble includes:
# percent_t - from 0 to 1 (for plotting) 
# rec_t, rec_f - timestamps and frequency for recording
# ref_t, ref_f, ref_i - timestamps, frequency, intensity for reference
get_voice_comparison_data <- function(recording_id) {
  # For each vocal recording
  my_record <- slice(voice_data, recording_id)
  # Get the pitch tier of the recording 
  my_record_pt <- vpts_interp[[recording_id]]
  
  # Get the TextGrid for the reference
  my_ref_tg <- get_ref_tg(my_record$phrase)
  
  # Get the start and end of the voiced section as a percentage
  start_percent <- my_ref_tg$syll$t1[1]/my_ref_tg$tmax
  end_percent <- tail(my_ref_tg$syll$t1, 1)/my_ref_tg$tmax
  
  # Get the interpolated frequency data of the phrase
  my_ref <- get_ref_pts_interp(my_record$phrase)
  my_ref_length <- length(my_ref$f)
  # Get the start and index of the reference phrase based on start/end percent from the textgrid
  my_ref_start_index <- floor(start_scrub * my_ref_length)
  my_ref_end_index <- ceiling(end_scrub * my_ref_length)
  # Get the part of the reference to compare with the recording
  my_ref_f <- my_ref$f[my_ref_start_index:my_ref_end_index]
  my_ref_t <- my_ref$t[my_ref_start_index:my_ref_end_index]
  
  # Get the intensity too
  my_ref_i <- get_ref_tsi(my_record$phrase)[my_ref_start_index:my_ref_end_index]
  # Also get the prosogram f0 stylization for the reference
  my_ref_f_psg <- get_ref_psgs_interp(my_record$phrase)$f[my_ref_start_index:my_ref_end_index]
  
  # Reinterpolate the recording based on the number of points in the new reference
  my_record_interp2<-pt.interpolate(my_record_pt, seq(my_record_pt$t[1], tail(my_record_pt$t, 1), 
                                               length.out = length(my_ref_f)))
  
  data_to_compare <- tibble(subject=my_record$subject, phrase=my_record$phrase, condition=my_record$type,
                            percent_t = seq(0, 1, length.out = length(my_ref_f)), 
                            rec_t = my_record_interp2$t, rec_f = my_record_interp2$f, 
                            ref_t = my_ref_t, ref_f = my_ref_f, ref_f_psg = my_ref_f_psg, ref_i = my_ref_i)
  return(data_to_compare)
}

# Put all the voice comparison data in a list so we don't have to regeneate it each time
voice_comparison_data <- lapply(voice_data$id, get_voice_comparison_data)
voice_comparison_data_tibble <- bind_rows(voice_comparison_data) %>% mutate(native = subject %in% natives$id)  

saveRDS(voice_comparison_data_tibble, "../data/21_02-study/saved/voice_curves.rds")

```

# Plotting a single recording with its reference
```{r}
rec_id <- 49
my_data <- voice_comparison_data[[rec_id]]
rec_info <- slice(voice_data, rec_id)
# Plot
ggplot(data=my_data, aes(x=percent_t, y=ref_f, color="Reference")) +
            scale_color_discrete(name = "Data source") +
            geom_line(size=1.5) + 
            geom_line(data=my_data, mapping=aes(x=percent_t, y=ref_f_psg, color="Prosogram Reference")) +
            geom_line(data=my_data, mapping=aes(x=percent_t, y=rec_f, color="Recording")) +
            labs(title=paste0("Reference and Recording - ", rec_id),
                 subtitle=paste(rec_info$phrase, rec_info$subject, rec_info$type),
                  y="Fequency (Semitones)", x="Percent time")
 
```

# Functions to compare
```{r}
library("wCorr") # for weighted correlation
library("mltools") # For weighted RMSE

# Get correlation, comparing with both the original and the prosogram vesions
corr_voice <- do.call("rbind", lapply(voice_data$id, function(elt) {
  # Get the data to compare
  data <- voice_comparison_data[[elt]]
  weightedCorr(data$ref_f, data$rec_f, method = "Pearson", 
                      weights = data$ref_i, ML = FALSE, fast = TRUE)
}))[,1]

corr_voice_psg <- do.call("rbind", lapply(voice_data$id, function(elt) {
  # Get the data to compare
  data <- voice_comparison_data[[elt]]
  weightedCorr(data$ref_f_psg, data$rec_f, method = "Pearson", 
                      weights = data$ref_i, ML = FALSE, fast = TRUE)
}))[,1]

#RSME
rmse_voice <- do.call("rbind", lapply(voice_data$id, function(elt) {
  # Get the data to compare
  data <- voice_comparison_data[[elt]]
  rmse(preds=data$ref_f, actuals=data$rec_f, weights=data$ref_i)
}))[,1]

rmse_voice_psg <- do.call("rbind", lapply(voice_data$id, function(elt) {
  # Get the data to compare
  data <- voice_comparison_data[[elt]]
  rmse(preds=data$ref_f_psg, actuals=data$rec_f, weights=data$ref_i)
}))[,1]

voice_data <- voice_data %>% add_column(corr = corr_voice, corr_psg = corr_voice_psg,
                                        rmse = rmse_voice, rmse_psg = rmse_voice_psg)

```



```{r}
require(gridExtra)

# Native speakers
natives <- subjects %>% filter(lvl_french == "N")

learners_voice <- voice_data %>% filter(!(subject %in% natives$id)) %>%
  ggplot(aes(x=subject, y=corr_psg, fill=factor(type))) +
  geom_boxplot(show.legend = FALSE) + 
  labs(title="Correlation of vocal recordings with reference", subtitle="Non-native", y="Correlation")

natives_voice <- voice_data %>% filter(subject %in% natives$id) %>%
  ggplot(aes(x=subject, y=corr_psg, fill=factor(type))) +
  geom_boxplot(show.legend = FALSE) +
  labs(title="", subtitle="Native", y="Correlation")

grid.arrange(learners_voice, natives_voice, ncol=2)
```

The good news is that imitation has higher scores in general. The bad news is that these scores actually lower than those of the gestures, which doesn't make much sense.

```{r}
# Native speakers
natives <- subjects %>% filter(lvl_french == "N")

learners_voice <- voice_data %>% filter(!(subject %in% natives$id)) %>%
  ggplot(aes(x=subject, y=rmse_psg, fill=factor(type))) +
  geom_boxplot(show.legend = FALSE) + 
  labs(title="RMSE of vocal recordings with reference", subtitle="Non-native", y="RSME")

natives_voice <- voice_data %>% filter(subject %in% natives$id) %>%
  ggplot(aes(x=subject, y=rmse_psg, fill=factor(type))) +
  geom_boxplot(show.legend = TRUE) +
  labs(title="", subtitle="Native", y="RSME")

grid.arrange(learners_voice, natives_voice, ncol=2)
```


# Plot all the pitch curves for each phrase

Let's see what's happening by plotting the curves

```{r}
# Plot gesture with its attempts
#plot_ref_with_recordings <- function(phrase_id, my_type='guide') {
# s <- "s10"
# phrase_id <- 7
# my_type <- "imitation"
# 
# #
# my_data <- filter(voice_comparison_data_tibble,phrase==ref_phrases[[phrase_id]], condition==my_type)
# #my_data <- filter(voice_comparison_data_tibble,subject==s,phrase==ref_phrases[[phrase_id]], condition==my_type)
# my_plot <- ggplot(data=my_data, aes(x=percent_t, y=ref_f_psg, color="Reference")) +
#             scale_color_discrete(name = "Data source") +
#             geom_line(size=2) + 
#             geom_line(mapping=aes(x=percent_t, y=rec_f, color=subject)) +
#             labs(title=paste0("Reference and gesture - All Subjects"),
#                  subtitle=paste0("Phrase: ", ref_phrases[[phrase_id]], ", Condition: ",  my_type), 
#                  y="Frequency (Semitones)", x="Percent time")
# my_plot
# 
# plot_filename <- paste0(ref_phrases[[phrase_id]], "-", my_type, ".png")
##ggsave(plot_filename,plot = my_plot, device='png', path="../data/21_02-study/_plots")
 
```

```{r}

natives <- subjects %>% filter(lvl_french == "N")
learners <- subjects %>% filter(lvl_french != "N")

# Function to plot the f0 curves with reference and save the result
plot_voices_with_ref <- function(my_data, ref_phrase, my_type, subject_group, subject_group_name) {
  my_data <- my_data %>% filter(subject %in% subject_group$id,phrase==ref_phrase,condition==my_type)
  my_plot <- ggplot(data=my_data, aes(x=percent_t, y=ref_f_psg, color="Reference")) +
            scale_color_discrete(name = "Data source") +
            geom_line(size=2) + ylim (-8, 12) +
            geom_line(mapping=aes(x=percent_t, y=rec_f, color=subject)) +
            labs(title=paste0("Reference and gesture - ", subject_group_name),
                 subtitle=paste0("Phrase: ", ref_phrase, "\nCondition: ",  my_type), 
                 y="Frequency (Semitones)", x="Percent time")
  
  plot_filename <- paste0(ref_phrase, "-", my_type, "-", subject_group_name,".png")
  ggsave(plot_filename, plot = my_plot, device='png', path="../data/21_02-study/_plots", 
            width = 6, height = 4)
  
  return(my_plot)
}

plot_voices_with_ref(voice_comparison_data_tibble, ref_phrases[[4]], "lecture", learners, "Learners")

sapply(ref_phrases, function(phrase) {
  #plot_voices_with_ref(voice_comparison_data_tibble, phrase, "imitation", natives, "Natives")
  #plot_voices_with_ref(voice_comparison_data_tibble, phrase, "imitation", learners, "Learners")
  plot_voices_with_ref(voice_comparison_data_tibble, phrase, "lecture", natives, "Natives")
  plot_voices_with_ref(voice_comparison_data_tibble, phrase, "lecture", learners, "Learners")
})


```


# Frequency only analysis

Dynamic time warping - figuring out the settings. Seems like asymmetric step and open.begin give best alignment

```{r}
library("dtw")
voice_id <- 181
recording_info <- slice(voice_data, voice_id)

my_recording <-voice_comparison_data[[voice_id]]
template <- my_recording$rec_f
reference <- my_recording$ref_f
alignment<-dtw(reference, template,keep=TRUE,step=asymmetric, open.begin = TRUE)

alignment2<-dtw(reference, template,keep=TRUE,window.type=sakoeChibaWindow,
window.size=100)

#plot(alignment, type="twoway")

dtw_voice <- sapply(alignment$index2, function(i) {template[i]})
dtw_ref <- sapply(alignment$index1, function(i) {reference[i]})

dtw_voice2 <- sapply(alignment2$index2, function(i) {template[i]})
dtw_ref2 <- sapply(alignment2$index1, function(i) {template[i]})

cdata <- tibble(ref_f=dtw_ref, rec_f=dtw_voice, percent=seq(1, length(ref_f))/length(ref_f))
cdata2 <-tibble(ref_f2=dtw_ref2, rec_f2=dtw_voice2, percent=seq(1, length(ref_f2))/length(ref_f2))

ggplot(data=cdata, aes(x=percent, y=ref_f, color="Reference")) +
            scale_color_discrete(name = "Data source") +
            geom_line(size=2) + 
            geom_line(mapping=aes(x=percent, y=rec_f, color="Warped Recording")) +
            geom_line(data=my_recording, mapping=aes(x=percent_t, y=rec_f, color="Original Recording")) +
            geom_line(data=cdata2, mapping=aes(x=percent, y=rec_f2, color="Warped Recording, symmetric")) +
             labs(title=paste("Reference and gesture for", recording_info$phrase),
                 subtitle=paste("Subject:", recording_info$subject, "- Condition:", recording_info$type), 
                 y="Frequency (Semitones)", x="Percent time")

```


# Prepare DTW data
```{r}
aligned_voice_data <- lapply(voice_data$id, function(elt) {
  my_record <- slice(voice_data, elt)
  
  data<-voice_comparison_data[[elt]]
  recording<- data$rec_f
  reference<-data$ref_f
  intensity<-data$ref_i
  # Alignement with original reference
  alignment<-dtw(reference, recording, keep=TRUE, step=asymmetric, open.begin=TRUE)
  
  # Alignment with prosogram
  reference_psg<-data$ref_f_psg
  alignment_psg <- dtw(reference_psg, recording,keep=TRUE, step=asymmetric, open.begin=TRUE)
  
  aligned_data <- tibble(subject=my_record$subject, phrase=my_record$phrase, condition=my_record$condition,
                         percent_t = seq(0, 1, length.out = length(alignment$index1)),
                         ref_f = reference[alignment$index1],
                         ref_i= intensity[alignment$index1],
                         rec_f = recording[alignment$index2],
                         ref_f_psg = reference_psg[alignment_psg$index1],
                         rec_f_psg = reference_psg[alignment_psg$index2],
                         ref_i_psg = intensity[alignment_psg$index2])
  aligned_data
})

aligned_voice_data_tibble <- bind_rows(aligned_voice_data) %>% mutate(native = subject %in% natives$id)  
saveRDS(aligned_voice_data_tibble, "../data/21_02-study/saved/voice_curves_aligned.rds")
```

# PLOT DTW

```{r}

phrase_id <- 2
ref_phrase <- ref_phrases[[phrase_id]]
c <- "imitation"

plot_voices_with_ref_aligned <- function(my_data, ref_phrase, my_type, subject_group, subject_group_name) {

  my_plot <- my_data %>% filter(subject %in% subject_group$id,phrase==ref_phrase,condition==my_type) %>%
            ggplot(my_data, mapping=aes(x=percent_t, y=ref_f, color="Reference")) +
            scale_color_discrete(name = "Data source") +
            geom_line(size=2) + ylim (-8, 12) +
            geom_line(mapping=aes(x=percent_t, y=rec_f, color=subject)) +
            labs(title=paste0("Aligned reference and voice, ", subject_group_name),
                 subtitle=paste0("Phrase: ", ref_phrase, "\nCondition: ",  my_type), 
                 y="Frequency (Semitones)", x="Percent time")

  plot_filename <- paste0(ref_phrase, "-", my_type, "-", subject_group_name,"-ALIGNED.png")
  ggsave(plot_filename, plot = my_plot, device='png', path="../data/21_02-study/_plots", 
            width = 6, height = 4)
  
  return(my_plot)
}

plot_voices_with_ref_aligned(aligned_voice_data_tibble, ref_phrases[[4]], "lecture", learners, "Learners")

# Plot and save all
sapply(ref_phrases, function(phrase) {
  plot_voices_with_ref_aligned(aligned_voice_data_tibble, phrase, "imitation", natives, "Natives")
  plot_voices_with_ref_aligned(aligned_voice_data_tibble, phrase, "imitation", learners, "Learners")
  plot_voices_with_ref_aligned(aligned_voice_data_tibble, phrase, "lecture", natives, "Natives")
  plot_voices_with_ref_aligned(aligned_voice_data_tibble, phrase, "lecture", learners, "Learners")
})
```




```{r}
# Get correlation, comparing with both the original and the prosogram vesions
corr_voice_notiming <- do.call("rbind", lapply(voice_data$id, function(elt) {
  # Get the data to compare
  data <- aligned_voice_data[[elt]]
  weightedCorr(data$ref_f, data$rec_f, method = "Pearson", 
                      weights = data$ref_i, ML = FALSE, fast = TRUE)
}))[,1]

corr_voice_psg_notiming <- do.call("rbind", lapply(voice_data$id, function(elt) {
  # Get the data to compare
  data <- aligned_voice_data[[elt]]
  weightedCorr(data$ref_f_psg, data$rec_f_psg, method = "Pearson", 
                      weights = data$ref_i_psg, ML = FALSE, fast = TRUE)
}))[,1]

#RSME
rmse_voice_notiming <- do.call("rbind", lapply(voice_data$id, function(elt) {
  # Get the data to compare
  data <- aligned_voice_data[[elt]]
  rmse(preds=data$ref_f, actuals=data$rec_f, weights=data$ref_i)
}))[,1]

rmse_voice_psg_notiming <- do.call("rbind", lapply(voice_data$id, function(elt) {
  # Get the data to compare
  data <- aligned_voice_data[[elt]]
  rmse(preds=data$ref_f_psg, actuals=data$rec_f, weights=data$ref_i)
}))[,1]

voice_data <- voice_data %>% add_column(corr_notiming = corr_voice_notiming, corr_psg_notiming = corr_voice_psg_notiming,
                                        rmse_notiming = rmse_voice_notiming, rmse_psg_notiming = rmse_voice_psg_notiming)
saveRDS(voice_data, "../data/21_02-study/saved/voice_scores.rds")
```
  
  